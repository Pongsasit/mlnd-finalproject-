{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data ; depressed tweet\n",
    "#Feature Engineering; ' Glove'\n",
    "#model\n",
    "#- simple RNN\n",
    "#- simple LSTM\n",
    "\n",
    "\n",
    "#evaluating\n",
    "#- precision\n",
    "#- recall\n",
    "#- F-1 score\n",
    "#- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pyprind\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10314, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            message  label\n",
       "0         106  just had a real good moment. i missssssssss hi...      0\n",
       "1         217         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2         220  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3         288  @lapcat Need to send 'em to my accountant tomo...      0\n",
       "4         540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_csv('C:/Users/user/Desktop/mlmd/sentiment_tweets3.csv')\n",
    "print(main_df.shape)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label\n",
       "0  just had a real good moment. i missssssssss hi...      0\n",
       "1         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3  @lapcat Need to send 'em to my accountant tomo...      0\n",
       "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = main_df[[\"message\", \"label\"]]\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8000\n",
       "1    2314\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_class = main_df.loc[main_df.label == 0, :]\n",
    "depress_class = main_df.loc[main_df.label == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting test and train \n",
    "test_normal = normal_class.iloc[:int(0.2*(len(normal_class))), :]\n",
    "test_depress = depress_class.iloc[:int(0.2*(len(depress_class))), :]\n",
    "\n",
    "train_normal = normal_class.iloc[int(0.2*(len(normal_class))):, :]\n",
    "train_depress = depress_class.iloc[int(0.2*(len(depress_class))):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8252, 2)\n",
      "(2062, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train_normal, train_depress], axis=0)\n",
    "print(train.shape)\n",
    "\n",
    "test = pd.concat([test_normal, test_depress], axis=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6400\n",
       "1    1852\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1600\n",
       "1     462\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"C:/Users/user/Desktop/mlmd/torchtext_data/train.csv\", index=False)\n",
    "test.to_csv(\"C:/Users/user/Desktop/mlmd/torchtext_data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==1.8.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/aa/755943184fbde85b77f52f1f9f52621fdcdda67f74db4f60880e3468ee8b/spacy-1.8.2.tar.gz (3.3MB)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from spacy==1.8.2) (1.16.4)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/53/1f428861e59c2382e22b8839d03cc315e1a7633a827497b3d389b8d8772d/murmurhash-0.26.4.tar.gz\n",
      "Collecting cymem<1.32,>=1.30 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/c4/cb92b8a267c17c02245f720ced4220a775a5eda58a9fdf7a46caec4eba53/cymem-1.31.2-cp36-cp36m-win_amd64.whl\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/4a/52cc241feda9392027e00abd2ea0d32d5b90d9e0d82e3c0f6f9ff8cd8ef9/preshed-1.0.1-cp36-cp36m-win_amd64.whl (71kB)\n",
      "Collecting thinc<6.6.0,>=6.5.0 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/9b/78fab962e0c8b55e3a745ebf2458708dfc7922c55eca3a9bff0233b25294/thinc-6.5.2.tar.gz (926kB)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from spacy==1.8.2) (1.12.0)\n",
      "Collecting pathlib (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
      "Collecting ujson>=1.35 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/50/d7/dce3fc97329639bba9c8b3cdadaa51ae2757c9402d4b43ac5feb8b624792/ujson-2.0.3.tar.gz (7.1MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting dill<0.3,>=0.2 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from spacy==1.8.2) (2.22.0)\n",
      "Collecting regex==2017.4.5 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/0b/c1c5781a707e6ea01bcf57d8ad3c42125260fca67ef79206ecaef04a8754/regex-2017.04.05-cp36-none-win_amd64.whl (243kB)\n",
      "Collecting ftfy<5.0.0,>=4.4.2 (from spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy==1.8.2) (1.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy==1.8.2) (4.44.1)\n",
      "Collecting cytoolz<0.9,>=0.8 (from thinc<6.6.0,>=6.5.0->spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/e6/ccc124714dcc1bd511e64ddafb4d5d20ada2533b92e3173a4cf09e0d0831/cytoolz-0.8.2.tar.gz (386kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy==1.8.2) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==1.8.2) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==1.8.2) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==1.8.2) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==1.8.2) (3.0.4)\n",
      "Requirement already satisfied: html5lib in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.8.2) (0.9999999)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.8.2) (0.1.7)\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy==1.8.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/8e/037b9ba5c6a5739ef0dcde60578c64d49f45f64c5e5e886531bfbc39157f/toolz-0.10.0.tar.gz (49kB)\n",
      "Building wheels for collected packages: ujson\n",
      "  Building wheel for ujson (PEP 517): started\n",
      "  Building wheel for ujson (PEP 517): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\63\\08\\c9\\4598408c0a93c6a4b008de44bd2924b264d1ed6390c5e17c03\n",
      "Successfully built ujson\n",
      "Building wheels for collected packages: spacy, murmurhash, thinc, pathlib, dill, ftfy, cytoolz, toolz\n",
      "  Building wheel for spacy (setup.py): started\n",
      "  Building wheel for spacy (setup.py): still running...\n",
      "  Building wheel for spacy (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\d7\\65\\03\\f64f9b14300b59cd5b473cf243d5390ee8b4cc25354c41ff0d\n",
      "  Building wheel for murmurhash (setup.py): started\n",
      "  Building wheel for murmurhash (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\90\\af\\51\\9efd49862c6dcb6439baaa235714fc4de5cecf3e01613b2fef\n",
      "  Building wheel for thinc (setup.py): started\n",
      "  Building wheel for thinc (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\1f\\36\\8b\\306d475aa414ff9fcec211da3da5d5e59582219d57f7ea9fa1\n",
      "  Building wheel for pathlib (setup.py): started\n",
      "  Building wheel for pathlib (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\f9\\b2\\4a\\68efdfe5093638a9918bd1bb734af625526e849487200aa171\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\5b\\d7\\0f\\e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "  Building wheel for ftfy (setup.py): started\n",
      "  Building wheel for ftfy (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\37\\54\\00\\d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "  Building wheel for cytoolz (setup.py): started\n",
      "  Building wheel for cytoolz (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\f8\\b1\\86\\c92e4d36b690208fff8471711b85eaa6bc6d19860a86199a09\n",
      "  Building wheel for toolz (setup.py): started\n",
      "  Building wheel for toolz (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\e1\\8b\\65\\3294e5b727440250bda09e8c0153b7ba19d328f661605cb151\n",
      "Successfully built spacy murmurhash thinc pathlib dill ftfy cytoolz toolz\n",
      "Installing collected packages: murmurhash, cymem, preshed, toolz, cytoolz, plac, dill, pathlib, thinc, ujson, regex, ftfy, spacy\n",
      "Successfully installed cymem-1.31.2 cytoolz-0.8.2 dill-0.2.9 ftfy-4.4.3 murmurhash-0.26.4 pathlib-1.0.1 plac-0.9.6 preshed-1.0.1 regex-2017.4.5 spacy-1.8.2 thinc-6.5.2 toolz-0.10.0 ujson-2.0.3\n",
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==1.8.2\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Status on system is True\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "print(\"Cuda Status on system is {}\".format(is_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok for tok in nltk.word_tokenize(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\")\n",
    "LABEL = data.LabelField(dtype=torch.long, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path=\"C:/Users/user/Desktop/mlmd/torchtext_data/\", train=\"train.csv\", test=\"test.csv\",format=\"csv\", skip_header=True, \n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8252\n",
      "Number of testing examples: 2062\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.840B.300d.zip: 2.18GB [38:39, 939kB/s]                                                             \n",
      "100%|█████████████████████████████████████████████████████████████████████▉| 2195471/2196017 [05:32<00:00, 6896.54it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, vectors=torchtext.vocab.GloVe(name='840B', dim=300), \n",
    "                 max_size=20000, min_freq=10)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 1330\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 4440),\n",
       " ('!', 4408),\n",
       " (',', 3406),\n",
       " ('I', 3117),\n",
       " ('to', 3048),\n",
       " (' ', 3010),\n",
       " ('the', 2817),\n",
       " ('a', 2388),\n",
       " ('and', 2084),\n",
       " ('you', 1949)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# keep in mind the sort_key option \n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), sort_key=lambda x: len(x.Text),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 6400, '1': 1852})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1330, 300])\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 374\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "model1 = simpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.embedding.weight.data = pretrained_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1.0, 15.0]).cuda()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model1.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
    "    correct = (ind == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, iterator, optimizer, criterion):\n",
    "    for epoch in range(1,epochs+1):\n",
    "    \n",
    "        training_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.Text).squeeze(0)\n",
    "        #         print(predictions.shape, batch.Label.shape, model(batch.Text).shape)\n",
    "            loss = criterion(predictions, batch.Label)\n",
    "        #         print(loss.shape)\n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "        training_loss /= len(iterator)\n",
    "        epoch_acc /= len(iterator)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {training_loss:.3f}, Train Acc: {epoch_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 0.561, Train Acc: 28.08% \n",
      "Epoch: 03, Train Loss: 0.516, Train Acc: 22.85% \n",
      "Epoch: 04, Train Loss: 0.514, Train Acc: 24.24% \n",
      "Epoch: 05, Train Loss: 0.512, Train Acc: 22.92% \n",
      "Epoch: 06, Train Loss: 0.507, Train Acc: 24.43% \n",
      "Epoch: 07, Train Loss: 0.508, Train Acc: 23.28% \n",
      "Epoch: 08, Train Loss: 0.508, Train Acc: 22.85% \n",
      "Epoch: 09, Train Loss: 0.513, Train Acc: 23.07% \n",
      "Epoch: 10, Train Loss: 0.501, Train Acc: 22.60% \n",
      "Epoch: 11, Train Loss: 0.517, Train Acc: 24.27% \n"
     ]
    }
   ],
   "source": [
    "train(10,model1,train_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.840 | Test Acc: 23.08% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.Text).squeeze(0)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            bar.update()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate(model1, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok for tok in sentence.split()]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.max(F.softmax(prediction.squeeze(0), dim=-1), 1)\n",
    "#     print(preds)\n",
    "    return preds, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1600\n",
      "           1       0.22      1.00      0.37       462\n",
      "\n",
      "    accuracy                           0.22      2062\n",
      "   macro avg       0.11      0.50      0.18      2062\n",
      "weighted avg       0.05      0.22      0.08      2062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"C:/Users/user/Desktop/mlmd/torchtext_data/test.csv\")\n",
    "pre = [predict_sentiment(k,model1)[1].item() for k in test.message]\n",
    "print(classification_report(test.label, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super(lstmRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_size, num_layers = 1)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden,_) = self.encoder(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        preds = self.fc(hidden.squeeze(0))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lstmRNN(HIDDEN_DIM,EMBEDDING_DIM,INPUT_DIM)\n",
    "model2 = model2.to(device)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "\n",
    "def train2(epochs, model, iterator, optimizer, criterion):\n",
    "    for epoch in range(1,epochs+1):\n",
    "    \n",
    "        training_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(iterator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.Text)\n",
    "        #         print(predictions.shape, batch.Label.shape, model(batch.Text).shape)\n",
    "            loss = criterion(predictions, batch.Label)\n",
    "        #         print(loss.shape)\n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.data.item()*batch.Text.size(0)\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "        training_loss /= len(iterator)\n",
    "        epoch_acc /= len(iterator)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {training_loss:.3f}, Train Acc: {epoch_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 13.154, Train Acc: 66.39% \n",
      "Epoch: 03, Train Loss: 2.571, Train Acc: 98.07% \n",
      "Epoch: 04, Train Loss: 1.424, Train Acc: 99.04% \n",
      "Epoch: 05, Train Loss: 0.571, Train Acc: 99.43% \n",
      "Epoch: 06, Train Loss: 0.379, Train Acc: 99.62% \n",
      "Epoch: 07, Train Loss: 0.208, Train Acc: 99.72% \n",
      "Epoch: 08, Train Loss: 0.129, Train Acc: 99.81% \n",
      "Epoch: 09, Train Loss: 0.116, Train Acc: 99.88% \n",
      "Epoch: 10, Train Loss: 0.087, Train Acc: 99.87% \n",
      "Epoch: 11, Train Loss: 0.087, Train Acc: 99.89% \n",
      "Epoch: 12, Train Loss: 0.228, Train Acc: 99.85% \n",
      "Epoch: 13, Train Loss: 0.586, Train Acc: 99.33% \n",
      "Epoch: 14, Train Loss: 0.585, Train Acc: 99.47% \n",
      "Epoch: 15, Train Loss: 0.139, Train Acc: 99.85% \n",
      "Epoch: 16, Train Loss: 0.241, Train Acc: 99.61% \n",
      "Epoch: 17, Train Loss: 0.147, Train Acc: 99.85% \n",
      "Epoch: 18, Train Loss: 0.067, Train Acc: 99.93% \n",
      "Epoch: 19, Train Loss: 0.055, Train Acc: 99.95% \n",
      "Epoch: 20, Train Loss: 0.066, Train Acc: 99.95% \n",
      "Epoch: 21, Train Loss: 0.053, Train Acc: 99.96% \n",
      "Epoch: 22, Train Loss: 0.055, Train Acc: 99.85% \n",
      "Epoch: 23, Train Loss: 0.074, Train Acc: 99.94% \n",
      "Epoch: 24, Train Loss: 0.300, Train Acc: 99.69% \n",
      "Epoch: 25, Train Loss: 0.243, Train Acc: 99.71% \n",
      "Epoch: 26, Train Loss: 0.081, Train Acc: 99.95% \n",
      "Epoch: 27, Train Loss: 0.064, Train Acc: 99.95% \n",
      "Epoch: 28, Train Loss: 0.067, Train Acc: 99.95% \n",
      "Epoch: 29, Train Loss: 0.069, Train Acc: 99.92% \n",
      "Epoch: 30, Train Loss: 0.063, Train Acc: 99.90% \n",
      "Epoch: 31, Train Loss: 0.101, Train Acc: 99.84% \n",
      "Epoch: 32, Train Loss: 0.352, Train Acc: 99.70% \n",
      "Epoch: 33, Train Loss: 0.084, Train Acc: 99.95% \n",
      "Epoch: 34, Train Loss: 0.093, Train Acc: 99.84% \n",
      "Epoch: 35, Train Loss: 0.069, Train Acc: 99.99% \n",
      "Epoch: 36, Train Loss: 0.046, Train Acc: 99.90% \n",
      "Epoch: 37, Train Loss: 0.054, Train Acc: 99.98% \n",
      "Epoch: 38, Train Loss: 0.056, Train Acc: 99.95% \n",
      "Epoch: 39, Train Loss: 0.038, Train Acc: 99.90% \n",
      "Epoch: 40, Train Loss: 0.053, Train Acc: 99.94% \n",
      "Epoch: 41, Train Loss: 0.058, Train Acc: 99.94% \n",
      "Epoch: 42, Train Loss: 0.067, Train Acc: 99.99% \n",
      "Epoch: 43, Train Loss: 0.046, Train Acc: 99.83% \n",
      "Epoch: 44, Train Loss: 0.035, Train Acc: 99.87% \n",
      "Epoch: 45, Train Loss: 0.049, Train Acc: 99.95% \n",
      "Epoch: 46, Train Loss: 0.037, Train Acc: 99.90% \n",
      "Epoch: 47, Train Loss: 0.086, Train Acc: 99.95% \n",
      "Epoch: 48, Train Loss: 0.208, Train Acc: 99.67% \n",
      "Epoch: 49, Train Loss: 0.042, Train Acc: 99.87% \n",
      "Epoch: 50, Train Loss: 0.048, Train Acc: 99.98% \n",
      "Epoch: 51, Train Loss: 0.038, Train Acc: 99.94% \n",
      "Epoch: 52, Train Loss: 0.043, Train Acc: 99.92% \n",
      "Epoch: 53, Train Loss: 0.052, Train Acc: 99.90% \n",
      "Epoch: 54, Train Loss: 0.027, Train Acc: 99.95% \n",
      "Epoch: 55, Train Loss: 0.040, Train Acc: 99.99% \n",
      "Epoch: 56, Train Loss: 0.055, Train Acc: 99.90% \n",
      "Epoch: 57, Train Loss: 0.031, Train Acc: 99.94% \n",
      "Epoch: 58, Train Loss: 0.037, Train Acc: 99.94% \n",
      "Epoch: 59, Train Loss: 0.038, Train Acc: 99.96% \n",
      "Epoch: 60, Train Loss: 0.056, Train Acc: 99.89% \n",
      "Epoch: 61, Train Loss: 0.039, Train Acc: 99.96% \n",
      "Epoch: 62, Train Loss: 0.076, Train Acc: 99.99% \n",
      "Epoch: 63, Train Loss: 0.046, Train Acc: 99.83% \n",
      "Epoch: 64, Train Loss: 0.030, Train Acc: 99.82% \n",
      "Epoch: 65, Train Loss: 0.046, Train Acc: 99.88% \n",
      "Epoch: 66, Train Loss: 0.073, Train Acc: 99.93% \n",
      "Epoch: 67, Train Loss: 0.031, Train Acc: 99.85% \n",
      "Epoch: 68, Train Loss: 0.049, Train Acc: 99.90% \n",
      "Epoch: 69, Train Loss: 0.031, Train Acc: 99.95% \n",
      "Epoch: 70, Train Loss: 0.033, Train Acc: 99.89% \n",
      "Epoch: 71, Train Loss: 0.039, Train Acc: 99.94% \n",
      "Epoch: 72, Train Loss: 0.045, Train Acc: 99.82% \n",
      "Epoch: 73, Train Loss: 0.029, Train Acc: 99.89% \n",
      "Epoch: 74, Train Loss: 0.046, Train Acc: 99.90% \n",
      "Epoch: 75, Train Loss: 0.027, Train Acc: 99.93% \n",
      "Epoch: 76, Train Loss: 0.042, Train Acc: 99.99% \n",
      "Epoch: 77, Train Loss: 0.040, Train Acc: 99.83% \n",
      "Epoch: 78, Train Loss: 0.030, Train Acc: 99.92% \n",
      "Epoch: 79, Train Loss: 0.039, Train Acc: 99.94% \n",
      "Epoch: 80, Train Loss: 0.056, Train Acc: 99.90% \n",
      "Epoch: 81, Train Loss: 0.062, Train Acc: 99.96% \n",
      "Epoch: 82, Train Loss: 0.043, Train Acc: 99.87% \n",
      "Epoch: 83, Train Loss: 0.051, Train Acc: 99.83% \n",
      "Epoch: 84, Train Loss: 0.032, Train Acc: 99.85% \n",
      "Epoch: 85, Train Loss: 0.026, Train Acc: 99.92% \n",
      "Epoch: 86, Train Loss: 0.031, Train Acc: 99.96% \n",
      "Epoch: 87, Train Loss: 0.040, Train Acc: 99.94% \n",
      "Epoch: 88, Train Loss: 0.041, Train Acc: 99.94% \n",
      "Epoch: 89, Train Loss: 0.030, Train Acc: 99.88% \n",
      "Epoch: 90, Train Loss: 0.051, Train Acc: 99.93% \n",
      "Epoch: 91, Train Loss: 0.030, Train Acc: 99.85% \n",
      "Epoch: 92, Train Loss: 0.030, Train Acc: 99.87% \n",
      "Epoch: 93, Train Loss: 0.031, Train Acc: 99.93% \n",
      "Epoch: 94, Train Loss: 0.027, Train Acc: 99.98% \n",
      "Epoch: 95, Train Loss: 0.034, Train Acc: 99.85% \n",
      "Epoch: 96, Train Loss: 0.037, Train Acc: 99.89% \n",
      "Epoch: 97, Train Loss: 0.038, Train Acc: 99.85% \n",
      "Epoch: 98, Train Loss: 0.037, Train Acc: 99.98% \n",
      "Epoch: 99, Train Loss: 0.042, Train Acc: 99.89% \n",
      "Epoch: 100, Train Loss: 0.049, Train Acc: 99.90% \n",
      "Epoch: 101, Train Loss: 0.046, Train Acc: 99.90% \n"
     ]
    }
   ],
   "source": [
    "train2(100, model2, train_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.084 | Test Acc: 99.57% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1600\n",
      "           1       0.99      0.73      0.84       462\n",
      "\n",
      "    accuracy                           0.94      2062\n",
      "   macro avg       0.96      0.86      0.90      2062\n",
      "weighted avg       0.94      0.94      0.93      2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok for tok in sentence.split()]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.max(F.softmax(prediction, dim=-1), 1)\n",
    "#     print(preds)\n",
    "    return preds, ind\n",
    "\n",
    "pre = [predict_sentiment(k,model2)[1].item() for k in test.message]\n",
    "\n",
    "print(classification_report(test.label, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ULMFiT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
