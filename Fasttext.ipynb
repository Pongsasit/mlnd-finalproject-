{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pyprind\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Status on system is True\n",
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      ".vector_cache\\wiki.en.vec: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8252\n",
      "Number of testing examples: 2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.en.vec: 6.60GB [11:00, 9.99MB/s]                                                                    \n",
      "  0%|                                                                                      | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████████████████████████████████████████████████████████████████▉| 2518627/2519370 [06:23<00:00, 7005.77it/s]"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "print(\"Cuda Status on system is {}\".format(is_cuda))\n",
    "def tokenizer(text):\n",
    "    return [tok for tok in nltk.word_tokenize(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\")\n",
    "LABEL = data.LabelField(dtype=torch.long, sequential=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path=\"C:/Users/user/Desktop/mlmd/torchtext_data/\", train=\"train.csv\", test=\"test.csv\",format=\"csv\", skip_header=True, \n",
    "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
    ")\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=torchtext.vocab.FastText(language='en'), \n",
    "                 max_size=20000, min_freq=10)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 1330\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 4440),\n",
       " ('!', 4408),\n",
       " (',', 3406),\n",
       " ('I', 3117),\n",
       " ('to', 3048),\n",
       " (' ', 3010),\n",
       " ('the', 2817),\n",
       " ('a', 2388),\n",
       " ('and', 2084),\n",
       " ('you', 1949)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 6400, '1': 1852})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# keep in mind the sort_key option \n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), sort_key=lambda x: len(x.Text),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super(lstmRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_size, num_layers = 1)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
    "    correct = (ind == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc\n",
    "def train(epochs, model, iterator, optimizer, criterion):\n",
    "    for epoch in range(1,epochs+1):\n",
    "    \n",
    "        training_loss = 0.0 \n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.Text).squeeze(0)\n",
    "    #         print(predictions.shape, batch.Label.shape, model(batch.Text).shape)\n",
    "            loss = criterion(predictions, batch.Label)\n",
    "    #         print(loss.shape)\n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        training_loss /= len(iterator)\n",
    "        epoch_acc /= len(iterator)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {training_loss:.3f}, Train Acc: {epoch_acc*100:.2f}% ')\n",
    "\n",
    "def train1(epochs, model, iterator, optimizer, criterion):\n",
    "    for epoch in range(1,epochs+1):\n",
    "    \n",
    "        training_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(iterator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.Text)\n",
    "        #         print(predictions.shape, batch.Label.shape, model(batch.Text).shape)\n",
    "            loss = criterion(predictions, batch.Label)\n",
    "        #         print(loss.shape)\n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "        training_loss /= len(iterator)\n",
    "        epoch_acc /= len(iterator)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {training_loss:.3f}, Train Acc: {epoch_acc*100:.2f}% ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.Text).squeeze(0)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            bar.update()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok for tok in sentence.split()]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.max(F.softmax(prediction, dim=-1), 1)\n",
    "#     print(preds)\n",
    "    return preds, ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1330, 300])\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 374\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "model1 = simpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model1.embedding.weight.data = pretrained_embeddings.cuda()\n",
    "class_weights = torch.tensor([1.0, 15.0]).cuda()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model1 = model1.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 0.535, Train Acc: 23.69% \n",
      "Epoch: 03, Train Loss: 0.524, Train Acc: 25.01% \n",
      "Epoch: 04, Train Loss: 0.539, Train Acc: 26.30% \n",
      "Epoch: 05, Train Loss: 0.515, Train Acc: 23.71% \n",
      "Epoch: 06, Train Loss: 0.527, Train Acc: 24.46% \n",
      "Epoch: 07, Train Loss: 0.522, Train Acc: 23.67% \n",
      "Epoch: 08, Train Loss: 0.527, Train Acc: 23.51% \n",
      "Epoch: 09, Train Loss: 0.511, Train Acc: 22.96% \n",
      "Epoch: 10, Train Loss: 0.528, Train Acc: 25.23% \n",
      "Epoch: 11, Train Loss: 0.501, Train Acc: 23.28% \n",
      "Epoch: 12, Train Loss: 0.504, Train Acc: 23.34% \n",
      "Epoch: 13, Train Loss: 0.519, Train Acc: 23.20% \n",
      "Epoch: 14, Train Loss: 0.502, Train Acc: 23.61% \n",
      "Epoch: 15, Train Loss: 0.508, Train Acc: 24.65% \n",
      "Epoch: 16, Train Loss: 0.512, Train Acc: 24.56% \n",
      "Epoch: 17, Train Loss: 0.540, Train Acc: 26.12% \n",
      "Epoch: 18, Train Loss: 0.512, Train Acc: 23.97% \n",
      "Epoch: 19, Train Loss: 0.507, Train Acc: 24.39% \n",
      "Epoch: 20, Train Loss: 0.514, Train Acc: 23.56% \n",
      "Epoch: 21, Train Loss: 0.517, Train Acc: 24.37% \n",
      "Epoch: 22, Train Loss: 0.521, Train Acc: 24.54% \n",
      "Epoch: 23, Train Loss: 0.508, Train Acc: 22.72% \n",
      "Epoch: 24, Train Loss: 0.509, Train Acc: 23.81% \n",
      "Epoch: 25, Train Loss: 0.510, Train Acc: 23.90% \n",
      "Epoch: 26, Train Loss: 0.499, Train Acc: 23.62% \n",
      "Epoch: 27, Train Loss: 0.507, Train Acc: 23.18% \n",
      "Epoch: 28, Train Loss: 0.513, Train Acc: 23.98% \n",
      "Epoch: 29, Train Loss: 0.510, Train Acc: 23.31% \n",
      "Epoch: 30, Train Loss: 0.516, Train Acc: 24.94% \n",
      "Epoch: 31, Train Loss: 0.504, Train Acc: 23.32% \n",
      "Epoch: 32, Train Loss: 0.508, Train Acc: 22.62% \n",
      "Epoch: 33, Train Loss: 0.511, Train Acc: 24.91% \n",
      "Epoch: 34, Train Loss: 0.532, Train Acc: 24.04% \n",
      "Epoch: 35, Train Loss: 0.521, Train Acc: 23.17% \n",
      "Epoch: 36, Train Loss: 0.512, Train Acc: 23.77% \n",
      "Epoch: 37, Train Loss: 0.514, Train Acc: 23.74% \n",
      "Epoch: 38, Train Loss: 0.507, Train Acc: 23.25% \n",
      "Epoch: 39, Train Loss: 0.513, Train Acc: 24.46% \n",
      "Epoch: 40, Train Loss: 0.518, Train Acc: 24.72% \n",
      "Epoch: 41, Train Loss: 0.509, Train Acc: 23.83% \n",
      "Epoch: 42, Train Loss: 0.512, Train Acc: 23.85% \n",
      "Epoch: 43, Train Loss: 0.517, Train Acc: 24.27% \n",
      "Epoch: 44, Train Loss: 0.504, Train Acc: 24.14% \n",
      "Epoch: 45, Train Loss: 0.505, Train Acc: 23.85% \n",
      "Epoch: 46, Train Loss: 0.507, Train Acc: 23.80% \n",
      "Epoch: 47, Train Loss: 0.508, Train Acc: 23.35% \n",
      "Epoch: 48, Train Loss: 0.521, Train Acc: 24.96% \n",
      "Epoch: 49, Train Loss: 0.502, Train Acc: 30.36% \n",
      "Epoch: 50, Train Loss: 0.526, Train Acc: 24.50% \n",
      "Epoch: 51, Train Loss: 0.510, Train Acc: 23.53% \n",
      "Epoch: 52, Train Loss: 0.502, Train Acc: 22.89% \n",
      "Epoch: 53, Train Loss: 0.520, Train Acc: 24.83% \n",
      "Epoch: 54, Train Loss: 0.480, Train Acc: 30.08% \n",
      "Epoch: 55, Train Loss: 0.514, Train Acc: 23.66% \n",
      "Epoch: 56, Train Loss: 0.522, Train Acc: 25.08% \n",
      "Epoch: 57, Train Loss: 0.497, Train Acc: 29.38% \n",
      "Epoch: 58, Train Loss: 0.467, Train Acc: 40.52% \n",
      "Epoch: 59, Train Loss: 0.482, Train Acc: 34.96% \n",
      "Epoch: 60, Train Loss: 0.463, Train Acc: 40.60% \n",
      "Epoch: 61, Train Loss: 0.487, Train Acc: 33.72% \n",
      "Epoch: 62, Train Loss: 0.516, Train Acc: 28.01% \n",
      "Epoch: 63, Train Loss: 0.516, Train Acc: 26.67% \n",
      "Epoch: 64, Train Loss: 0.513, Train Acc: 26.54% \n",
      "Epoch: 65, Train Loss: 0.510, Train Acc: 26.13% \n",
      "Epoch: 66, Train Loss: 0.503, Train Acc: 23.14% \n",
      "Epoch: 67, Train Loss: 0.501, Train Acc: 23.14% \n",
      "Epoch: 68, Train Loss: 0.523, Train Acc: 23.85% \n",
      "Epoch: 69, Train Loss: 0.510, Train Acc: 23.01% \n",
      "Epoch: 70, Train Loss: 0.507, Train Acc: 22.85% \n",
      "Epoch: 71, Train Loss: 0.499, Train Acc: 23.29% \n",
      "Epoch: 72, Train Loss: 0.511, Train Acc: 24.10% \n",
      "Epoch: 73, Train Loss: 0.493, Train Acc: 24.73% \n",
      "Epoch: 74, Train Loss: 0.495, Train Acc: 23.06% \n",
      "Epoch: 75, Train Loss: 0.521, Train Acc: 27.17% \n",
      "Epoch: 76, Train Loss: 0.517, Train Acc: 25.12% \n",
      "Epoch: 77, Train Loss: 0.503, Train Acc: 23.14% \n",
      "Epoch: 78, Train Loss: 0.513, Train Acc: 24.28% \n",
      "Epoch: 79, Train Loss: 0.496, Train Acc: 23.32% \n",
      "Epoch: 80, Train Loss: 0.500, Train Acc: 23.64% \n",
      "Epoch: 81, Train Loss: 0.499, Train Acc: 23.16% \n",
      "Epoch: 82, Train Loss: 0.504, Train Acc: 23.88% \n",
      "Epoch: 83, Train Loss: 0.509, Train Acc: 25.39% \n",
      "Epoch: 84, Train Loss: 0.495, Train Acc: 22.82% \n",
      "Epoch: 85, Train Loss: 0.505, Train Acc: 25.06% \n",
      "Epoch: 86, Train Loss: 0.514, Train Acc: 24.67% \n",
      "Epoch: 87, Train Loss: 0.484, Train Acc: 33.23% \n",
      "Epoch: 88, Train Loss: 0.497, Train Acc: 32.43% \n",
      "Epoch: 89, Train Loss: 0.518, Train Acc: 24.69% \n",
      "Epoch: 90, Train Loss: 0.508, Train Acc: 25.09% \n",
      "Epoch: 91, Train Loss: 0.498, Train Acc: 32.05% \n",
      "Epoch: 92, Train Loss: 0.439, Train Acc: 44.05% \n",
      "Epoch: 93, Train Loss: 0.531, Train Acc: 33.08% \n",
      "Epoch: 94, Train Loss: 0.516, Train Acc: 23.48% \n",
      "Epoch: 95, Train Loss: 0.507, Train Acc: 23.24% \n",
      "Epoch: 96, Train Loss: 0.513, Train Acc: 24.39% \n",
      "Epoch: 97, Train Loss: 0.520, Train Acc: 25.20% \n",
      "Epoch: 98, Train Loss: 0.506, Train Acc: 24.12% \n",
      "Epoch: 99, Train Loss: 0.503, Train Acc: 22.84% \n",
      "Epoch: 100, Train Loss: 0.502, Train Acc: 23.54% \n",
      "Epoch: 101, Train Loss: 0.510, Train Acc: 23.50% \n"
     ]
    }
   ],
   "source": [
    "train(100,model1,train_iterator, optimizer1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.322 | Test Acc: 31.63% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model1, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/user/Desktop/mlmd/torchtext_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment2(sentence,model):\n",
    "    tokenized = [tok for tok in sentence.split()]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.max(F.softmax(prediction.squeeze(0), dim=-1), 1)\n",
    "#     print(preds)\n",
    "    return preds, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.08      0.15      1600\n",
      "           1       0.24      0.98      0.38       462\n",
      "\n",
      "    accuracy                           0.28      2062\n",
      "   macro avg       0.59      0.53      0.26      2062\n",
      "weighted avg       0.78      0.28      0.20      2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre = [predict_sentiment2(k,model1)[1].item() for k in test.message]\n",
    "print(classification_report(test.label, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lstmRNN(HIDDEN_DIM,EMBEDDING_DIM,INPUT_DIM)\n",
    "model2 = model2.to(device)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 0.225, Train Acc: 72.03% \n",
      "Epoch: 03, Train Loss: 0.042, Train Acc: 99.18% \n",
      "Epoch: 04, Train Loss: 0.025, Train Acc: 99.24% \n",
      "Epoch: 05, Train Loss: 0.025, Train Acc: 99.41% \n",
      "Epoch: 06, Train Loss: 0.008, Train Acc: 99.67% \n",
      "Epoch: 07, Train Loss: 0.007, Train Acc: 99.67% \n",
      "Epoch: 08, Train Loss: 0.005, Train Acc: 99.77% \n",
      "Epoch: 09, Train Loss: 0.003, Train Acc: 99.90% \n",
      "Epoch: 10, Train Loss: 0.002, Train Acc: 99.89% \n",
      "Epoch: 11, Train Loss: 0.002, Train Acc: 99.84% \n",
      "Epoch: 12, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 13, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 14, Train Loss: 0.005, Train Acc: 99.82% \n",
      "Epoch: 15, Train Loss: 0.004, Train Acc: 99.85% \n",
      "Epoch: 16, Train Loss: 0.007, Train Acc: 99.65% \n",
      "Epoch: 17, Train Loss: 0.003, Train Acc: 99.82% \n",
      "Epoch: 18, Train Loss: 0.002, Train Acc: 99.94% \n",
      "Epoch: 19, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 20, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 21, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 22, Train Loss: 0.001, Train Acc: 99.93% \n",
      "Epoch: 23, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 24, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 25, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 26, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 27, Train Loss: 0.002, Train Acc: 99.98% \n",
      "Epoch: 28, Train Loss: 0.002, Train Acc: 99.94% \n",
      "Epoch: 29, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 30, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 31, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 32, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 33, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 34, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 35, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 36, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 37, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 38, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 39, Train Loss: 0.002, Train Acc: 99.98% \n",
      "Epoch: 40, Train Loss: 0.001, Train Acc: 99.87% \n",
      "Epoch: 41, Train Loss: 0.001, Train Acc: 99.87% \n",
      "Epoch: 42, Train Loss: 0.001, Train Acc: 99.92% \n",
      "Epoch: 43, Train Loss: 0.001, Train Acc: 99.85% \n",
      "Epoch: 44, Train Loss: 0.003, Train Acc: 99.87% \n",
      "Epoch: 45, Train Loss: 0.002, Train Acc: 99.76% \n",
      "Epoch: 46, Train Loss: 0.002, Train Acc: 99.90% \n",
      "Epoch: 47, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 48, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 49, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 50, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 51, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 52, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 53, Train Loss: 0.001, Train Acc: 99.92% \n",
      "Epoch: 54, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 55, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 56, Train Loss: 0.001, Train Acc: 99.88% \n",
      "Epoch: 57, Train Loss: 0.001, Train Acc: 99.94% \n",
      "Epoch: 58, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 59, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 60, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 61, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 62, Train Loss: 0.001, Train Acc: 99.94% \n",
      "Epoch: 63, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 64, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 65, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 66, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 67, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 68, Train Loss: 0.001, Train Acc: 99.83% \n",
      "Epoch: 69, Train Loss: 0.001, Train Acc: 99.83% \n",
      "Epoch: 70, Train Loss: 0.001, Train Acc: 99.84% \n",
      "Epoch: 71, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 72, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 73, Train Loss: 0.002, Train Acc: 99.99% \n",
      "Epoch: 74, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 75, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 76, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 77, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 78, Train Loss: 0.001, Train Acc: 99.93% \n",
      "Epoch: 79, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 80, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 81, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 82, Train Loss: 0.000, Train Acc: 99.93% \n",
      "Epoch: 83, Train Loss: 0.001, Train Acc: 99.98% \n",
      "Epoch: 84, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 85, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 86, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 87, Train Loss: 0.001, Train Acc: 99.89% \n",
      "Epoch: 88, Train Loss: 0.001, Train Acc: 99.92% \n",
      "Epoch: 89, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 90, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 91, Train Loss: 0.001, Train Acc: 99.92% \n",
      "Epoch: 92, Train Loss: 0.001, Train Acc: 99.92% \n",
      "Epoch: 93, Train Loss: 0.001, Train Acc: 99.90% \n",
      "Epoch: 94, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 95, Train Loss: 0.001, Train Acc: 99.99% \n",
      "Epoch: 96, Train Loss: 0.001, Train Acc: 99.84% \n",
      "Epoch: 97, Train Loss: 0.001, Train Acc: 99.96% \n",
      "Epoch: 98, Train Loss: 0.001, Train Acc: 99.93% \n",
      "Epoch: 99, Train Loss: 0.001, Train Acc: 99.95% \n",
      "Epoch: 100, Train Loss: 0.001, Train Acc: 99.82% \n",
      "Epoch: 101, Train Loss: 0.001, Train Acc: 99.96% \n"
     ]
    }
   ],
   "source": [
    "train1(100,model2,train_iterator, optimizer2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.062 | Test Acc: 99.71% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      1600\n",
      "           1       1.00      0.71      0.83       462\n",
      "\n",
      "    accuracy                           0.93      2062\n",
      "   macro avg       0.96      0.85      0.89      2062\n",
      "weighted avg       0.94      0.93      0.93      2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre = [predict_sentiment(k,model2)[1].item() for k in test.message]\n",
    "\n",
    "print(classification_report(test.label, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
